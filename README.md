# Cultural QA Agent - RAG System

A Retrieval-Augmented Generation (RAG) system for answering cultural questions about China, Iran, the United Kingdom, and the United States. Built with constraint of using only the `Meta-Llama-3-8B` model.

## âœ¨ Features

- ğŸ” **Multi-Source Retrieval**: Wikipedia, Wikivoyage, web search, and training data
- ğŸ¯ **Hybrid Search**: Dense (BGE-M3) + Sparse (BM25) retrieval
- ğŸ”„ **Advanced Reranking**: Cross-encoder (BGE-reranker-v2-m3) or Late-Interaction (ColBERT)
- ğŸ§  **LLM-based Query Generation**: Semantic query expansion using Llama-3-8B
- ğŸ“Š **Two Question Types**: Multiple Choice Questions (MCQ) and Short Answer Questions (SAQ)
- âš¡ **Async Workflow**: Event-driven architecture using LlamaIndex Workflows
- ğŸ“ˆ **MLflow Integration**: Comprehensive experiment tracking and metrics
- ğŸš€ **Multi-Process Inference**: Optimized batch prediction with configurable concurrency

## ğŸ“‹ Table of Contents

- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [Usage](#usage)
  - [Building the Index](#1-build-the-index)
  - [Running Evaluation](#2-run-evaluation)
  - [Generating Predictions](#3-generate-competition-submissions)
- [Configuration](#configuration)
- [Performance](#performance)
- [License](#license)

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ config.yaml                  # Hydra configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_dataset_mcq.csv        # MCQ training data
â”‚   â”œâ”€â”€ train_dataset_saq.csv        # SAQ training data
â”‚   â”œâ”€â”€ test_dataset_mcq.csv         # MCQ test data
â”‚   â”œâ”€â”€ test_dataset_saq.csv         # SAQ test data
â”‚   â”œâ”€â”€ mcq_prediction.tsv           # Generated MCQ predictions
â”‚   â””â”€â”€ saq_prediction.tsv           # Generated SAQ predictions
â”œâ”€â”€ src/culture_questions_agent/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ ingest.py                # Main ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ wikipedia.py             # Wikipedia data loader
â”‚   â”‚   â”œâ”€â”€ wikivoyage.py            # Wikivoyage XML parser
â”‚   â”‚   â”œâ”€â”€ questions.py             # Training data reader
â”‚   â”‚   â””â”€â”€ web.py                   # Web search & scraping
â”‚   â”œâ”€â”€ predictor/
â”‚   â”‚   â”œâ”€â”€ discriminative_predictor.py  # NLL-based prediction
â”‚   â”‚   â””â”€â”€ generative_predictor.py      # Text generation
â”‚   â”œâ”€â”€ workflow.py                  # Event-driven QA workflow
â”‚   â”œâ”€â”€ multi_retriever.py           # Multi-source retrieval orchestrator
â”‚   â”œâ”€â”€ query_generator.py           # LLM query generation
â”‚   â”œâ”€â”€ search_tools.py              # Web search integration
â”‚   â”œâ”€â”€ inference.py                 # Competition submission generation
â”‚   â”œâ”€â”€ evaluate.py                  # MLflow evaluation
â”‚   â”œâ”€â”€ data.py                      # Data loading utilities
â”‚   â””â”€â”€ structures.py                # Data structures
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ query_generation_prompt.jinja    # Query generation template
â”‚   â”œâ”€â”€ mcq_prompt.jinja                 # MCQ answering template
â”‚   â””â”€â”€ saq_prompt.jinja                 # SAQ answering template
â”œâ”€â”€ storage/lancedb/                 # Vector database storage
â”œâ”€â”€ tracking/                        # MLflow tracking
â”‚   â”œâ”€â”€ mlruns.sqlite                # Experiment database
â”‚   â””â”€â”€ artifacts/                   # Experiment artifacts
â”œâ”€â”€ notebooks/                       # Data exploration
â””â”€â”€ pyproject.toml                   # Poetry dependencies
```

## ğŸš€ Installation

### Prerequisites

- Python 3.11+
- Poetry
- CUDA-compatible GPU (recommended)

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd culture_questions_agent

# Install dependencies
poetry install

# Set cache directory (optional)
export HF_HOME=/path/to/cache
```

## âš¡ Quick Start

```bash
# 1. Build the knowledge base
poetry run python -m culture_questions_agent.ingestion.ingest

# 2. Evaluate on MCQ
poetry run python -m culture_questions_agent.evaluate task_type="mcq"

# 3. Generate competition submissions
poetry run python -m culture_questions_agent.inference task_type="mcq"

# 4. View results in MLflow
mlflow ui --backend-store-uri sqlite:///tracking/mlruns.sqlite
```

## ğŸ—ï¸ Architecture

### System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PHASE 1: DATA INGESTION                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼             â–¼             â–¼
            Wikipedia      Wikivoyage    Training Data
                    â”‚             â”‚             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                      Section-Aware Parsing
                                  â–¼
                      Semantic Chunking (256 tokens)
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  LanceDB Vector Store   â”‚
                    â”‚  â€¢ Dense: BGE-M3        â”‚
                    â”‚  â€¢ Sparse: BM25         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PHASE 2: QUESTION ANSWERING                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Question (MCQ/SAQ)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Query Generation    â”‚ â† Llama-3-8B (optional)
â”‚  â€¢ Semantic expansion   â”‚
â”‚  â€¢ Or direct question   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Multi-Retriever Orchestration (Parallel) â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Wikipedia  â”‚ Web Search â”‚ Train Data  â”‚  â”‚
â”‚  â”‚ â€¢ Dense    â”‚ â€¢ DDGS     â”‚ â€¢ Dense     â”‚  â”‚
â”‚  â”‚ â€¢ Sparse   â”‚            â”‚ â€¢ Sparse    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Deduplication       â”‚
â”‚  â€¢ By content hash      â”‚
â”‚  â€¢ Preserve top-k       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Grouped Reranking               â”‚
â”‚  â€¢ Web + Wiki: top-6 (ColBERT)      â”‚
â”‚  â€¢ Training Data: top-4 (BGE v2-m3) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Prediction          â”‚
â”‚  â€¢ MCQ: Logits          â”‚
â”‚  â€¢ SAQ: Generate        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      Answer
```

### Key Components

1. **Ingestion Pipeline** ([`src/culture_questions_agent/ingestion/`](src/culture_questions_agent/ingestion/))
   - Wikipedia section parser with metadata extraction
   - Wikivoyage XML dump processor
   - Web search with query generation
   - Training data indexing

2. **Retrieval System** ([`multi_retriever.py`](src/culture_questions_agent/multi_retriever.py))
   - Multi-source parallel retrieval
   - Hybrid search (dense + sparse)
   - Content-based deduplication
   - Fusion strategies

3. **Reranking** ([`workflow.py`](src/culture_questions_agent/workflow.py))
   - ColBERT (late-interaction) for Wikipedia + Web Search
   - BGE-reranker-v2-m3 (cross-encoder) for Training Data
   - Grouped reranking by source

4. **Prediction** ([`src/culture_questions_agent/predictor/`](src/culture_questions_agent/predictor/))
   - Discriminative: Logits-based (for MCQ)
   - Generative: Text generation (for SAQ)

## ğŸ“– Usage

### 1. Build the Index

Build the RAG knowledge base from multiple sources:

```bash
poetry run python -m culture_questions_agent.ingestion.ingest
```

**What happens:**
- Downloads Wikipedia pages for 4 countries (China, Iran, UK, US)
- Parses Wikivoyage XML dump
- Performs web searches based on training questions
- Creates semantic chunks (256 tokens, 50 overlap)
- Builds hybrid indices (Dense + Sparse)
- Saves to [`storage/lancedb/`](storage/lancedb/)

**Configuration:** Edit [`conf/config.yaml`](conf/config.yaml) to customize:
- `ingestion.country_filter_list`: Countries to include
- `ingestion.topic_templates`: Wikipedia page templates
- `ingestion.chunk_size`: Chunk size for splitting

### 2. Run Evaluation

Evaluate the system on training data:

**MCQ Evaluation:**
```bash
poetry run python -m culture_questions_agent.evaluate task_type="mcq"
```

**SAQ Evaluation:**
```bash
poetry run python -m culture_questions_agent.evaluate task_type="saq"
```

**Output:**
- Overall accuracy metrics
- Per-country accuracy breakdown
- MLflow experiment tracking

**View Results:**
```bash
mlflow ui --backend-store-uri sqlite:///tracking/mlruns.sqlite
# Open http://localhost:5000
```

### 3. Generate Competition Submissions

Generate prediction files for test datasets:

**MCQ Predictions:**
```bash
poetry run python -m culture_questions_agent.inference task_type="mcq"
# Output: data/mcq_prediction.tsv
```

**SAQ Predictions:**
```bash
poetry run python -m culture_questions_agent.inference task_type="saq"
# Output: data/saq_prediction.tsv
```

**Performance Optimization:**

Configure concurrency in [`conf/config.yaml`](conf/config.yaml):
```yaml
inference:
  max_concurrent: 10      # Concurrent predictions per process
  num_processes: 0        # Number of processes (0 = single process)
```

## âš™ï¸ Configuration

The system uses [Hydra](https://hydra.cc/) for configuration management. See [`conf/config.yaml`](conf/config.yaml) for all options.

### Key Configuration Sections

#### Model Settings
```yaml
model:
  llm_name: "meta-llama/Meta-Llama-3-8B"
  cache_dir: "/path/to/cache"
  reranker_name: "BAAI/bge-reranker-v2-m3"
  embedding_model_name: "BAAI/bge-m3"
  predictor_type: "generative"  # or "discriminative"
```

#### Retrieval Settings
```yaml
retrieval:
  use_colbert: true              # Late-interaction retrieval
  use_reranker: true             # Cross-encoder reranking
  use_wiki_retrieval: true       # Enable Wikipedia
  use_train_data_retrieval: true # Enable training data
  use_web_retrieval: true        # Enable web search
  num_queries: 3                 # Queries to generate
  use_direct_question: false     # Use question directly
```

#### Reranking Groups
```yaml
retrieval:
  reranking_groups:
    - sources: ["train_data"]
      top_k: 4
    - sources: ["wiki", "web"]
      top_k: 6
```

#### Ingestion Settings
```yaml
ingestion:
  chunk_size: 256
  chunk_overlap: 50
  skip_wiki: false
  skip_web: false
  skip_training_data: false
```

## ğŸ“Š Performance

**System Specifications:**
- Model: Meta-Llama-3-8B
- Embedding: BAAI/bge-m3
- Reranker: BAAI/bge-reranker-v2-m3
- Vector Store: LanceDB (hybrid search)

**Optimization Features:**
- Multi-process inference with configurable concurrency
- Async workflow for I/O operations
- Efficient batch processing
- GPU acceleration for embeddings and reranking

## ğŸ”§ Development

### Project Layout

The project follows a modular architecture:

- **Ingestion** ([`src/culture_questions_agent/ingestion/`](src/culture_questions_agent/ingestion/)): Data loading and indexing
- **Retrieval** ([`multi_retriever.py`](src/culture_questions_agent/multi_retriever.py)): Multi-source retrieval orchestration
- **Workflow** ([`workflow.py`](src/culture_questions_agent/workflow.py)): Event-driven QA pipeline
- **Prediction** ([`src/culture_questions_agent/predictor/`](src/culture_questions_agent/predictor/)): Answer generation strategies

### Key Files

- [`workflow.py`](src/culture_questions_agent/workflow.py): Main QA workflow with 4 steps (query generation, retrieval, reranking, prediction)
- [`multi_retriever.py`](src/culture_questions_agent/multi_retriever.py): Orchestrates parallel retrieval from multiple sources
- [`query_generator.py`](src/culture_questions_agent/query_generator.py): LLM-based query expansion
- [`inference.py`](src/culture_questions_agent/inference.py): Competition submission generation
- [`evaluate.py`](src/culture_questions_agent/evaluate.py): MLflow-based evaluation

## ğŸ“ License

MIT